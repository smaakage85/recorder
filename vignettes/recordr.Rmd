    
---
title: "Introduction to recordr"
author: "Lars Kjeldgaard"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to recordr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

`recordr` 0.9.0 is now available on CRAN. `recordr` is a lightweight toolkit to 
validate new observations when computing their corresponding predictions with a 
predictive model. 

With `recordr` the validation process consists of two steps: 

1. record relevant statistics and meta data of the variables from the original
training data for the predictive model 
2. use these data to run a set of basic validation tests on the new set of 
observations.

## Motivation

There can be many data specific reasons, why you might not be confident in the
predictions of a predictive model on new data. 

Some of them are obvious, e.g.:

* One or more variables in training data are not found in new data
* The class of a given variable differs in training data and new data

Others are more subtle, for instance when observations in 
new data are not in the "span" of the training data. One example of this could 
be, when a variable is "N/A" (missing) for a new observation to be predicted, 
but no missing values appeared for the same variable in the training data.
This implies, that the new observation is not within the "span" of the training 
data. Another way of putting this: the model has never encountered an 
observation like this before, therefore there is good reason to doubt the 
quality of its prediction computed with the predictive model.

## recordr Workflow

In order to validate new data (before computing their predictions with a 
preidictive model) relevant statistics and meta data of the variables must first 
be learned from the training data of the model. 



