---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
)
```

# recordr <img src="man/figures/logo.png" align="right" height=140/>

[![Travis-CI Build Status](https://travis-ci.org/smaakage85/customsteps.svg?branch=master)](https://travis-ci.org/smaakage85/recordr)
[![CRAN\_Release\_Badge](http://www.r-pkg.org/badges/version-ago/modelgrid)](https://CRAN.R-project.org/package=recordr) [![CRAN\_Download\_Badge](http://cranlogs.r-pkg.org/badges/modelgrid)](https://CRAN.R-project.org/package=recordr)

`recordr` is a lightweight toolkit to validate new observations when computing
their corresponding predictions with a predictive model. 

With `recordr` the validation process consists of two steps: 

1. record relevant statistics and meta data of the variables from the original
training data for the predictive model 
2. use these data to run a set of basic validation tests on the new set of 
observations.

## Motivation

There can be many data specific reasons, why you might not be confident in the
predictions of a predictive model on new data. 

Some of them are obvious, e.g.:

* One or more variables in training data are not found in new data
* The class of a given variable differs in training data and new data

Others are more subtle, for instance if it is the case, that observations in 
new data are not in the "span" of the training data. 

If one or more of the `recordr` validation tests fail on new data, you might not
be confident in the corresponding predictions.

## Installation

`recordr` can be installed from CRAN with `install.packages('recordr')`. 
If you want the development version then install directly from GitHub:

```{r, eval = FALSE}
devtools::install_github("smaakage85/recordr")
```

## Workflow Example

Get ready and load package.

```{r}
library(recordr)
```

The famous `iris` dataset will be used as an example. The data set is divided
into training data, that can be used for model development, and new data to be
predicted with the model at some point.

```{r}
set.seed(1)
trn_idx <- sample(seq_len(nrow(iris)), 100)
data_training <- iris[trn_idx, ]
data_new <- iris[-trn_idx, ]
```

Record statistics and meta data of the training data with `record()`.

```{r}
tape <- record(data_training)
```

Run validation tests on new data with `play()`.

```{r}
playback <- play(tape, data_new)
```

Print the over-all results of the validation tests.

```{r}
playback
```

The test summary tells us, that one observation (row #11) has a value of 
the variable "Petal.Length" outside the recorded range in training; hence we 
might not be confident in the prediction of this particular observation.

After running the validation tests, you can extract the results of (any) failed 
tests for the rows/observations of new data with `get_failed_tests()`.

```{r}
failed_tests <- get_failed_tests(playback)
# print.
library(knitr)
kable(head(failed_tests, 15))
```

You might also find the functions `get_failed_tests_string()` and 
`get_clean_rows()` to be useful.

That is basically it. If you to know more about all of the exciting features 
of `recordr`, take a look at the vignette.

Also, if you have any feedback on the package, please let me hear from you.

